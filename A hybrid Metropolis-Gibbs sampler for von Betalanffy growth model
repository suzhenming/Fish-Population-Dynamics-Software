#*************************************************************************************
#
# R Code for Bayesian estimation of von Beterlanfy growth equation
#
# Author: Dr. Zhenming Su
#
# suz@michigan.gov
# IFR, DNR, State of Michigan
# 2005-2007 (R) 
#
#*************************************************************************************

# Northern rockfish male data (from Quinn and Deriso 1999)
lvbdata <- data.frame(age = c(6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 8,
8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10,
10, 10, 11, 11, 11, 11, 11, 11, 11, 12, 12, 13, 13, 13, 13, 13,
13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16,
16, 16, 16, 16, 17, 17, 17, 18, 18, 20, 23, 24, 24, 24, 25, 26,
27, 27, 27, 29, 29, 33, 40, 6, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,
8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10,
10, 10, 10, 11, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,
13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15,
15, 15, 16, 16, 16, 16, 16, 17, 20, 22, 22, 22, 23, 23, 23, 23,
23, 23, 23, 24, 24, 24, 24, 25, 25, 27, 28, 29, 31, 32, 32, 32,
37, 40, 41, 43), Lobs = c(220, 270, 290, 290, 280, 290, 290, 310, 290, 300,
310, 320, 330, 280, 290, 310, 310, 320, 290, 300, 300, 320, 290,
310, 310, 310, 320, 340, 350, 300, 300, 310, 320, 320, 320, 320,
330, 310, 340, 340, 310, 320, 330, 310, 320, 330, 330, 350, 330,
300, 330, 350, 360, 360, 320, 330, 330, 370, 370, 330, 340, 350,
350, 330, 340, 330, 330, 360, 370, 340, 340, 370, 340, 350, 360,
320, 370, 340, 350, 350, 320, 360, 360, 340, 250, 260, 290, 270,
290, 290, 300, 320, 270, 280, 310, 270, 280, 290, 300, 300, 310,
310, 310, 310, 320, 300, 310, 330, 330, 270, 290, 290, 300, 310,
340, 310, 330, 310, 340, 330, 340, 330, 340, 360, 360, 360, 360,
370, 370, 370, 380, 310, 340, 350, 340, 340, 340, 360, 360, 370,
370, 380, 330, 350, 360, 360, 360, 380, 340, 370, 360, 360, 400,
350, 400, 350, 390, 360, 370, 360, 390, 370, 370, 370, 380, 380,
400, 360, 390, 360, 370, 370, 430, 400, 410, 360, 410, 390, 410,
390, 390, 410, 400))

attach(lvbdata)

require(mvtnorm)
require(MASS)


model <- function (yobs, xobs, para, sigma2)
{
  Lobs <- yobs
  nd <- length(yobs)
  age  <- xobs
  
  Linf <- exp(para[1])
  k <- exp(para[2])
  t0 <- para[3]

  Lpred = Linf * (1-exp(-k * (age - t0)))
  ssq  <-  sum((Lobs - Lpred)^2, na.rm = T)
  
  log.post  <-  (-nd*log(sigma2)/2 - ssq/(sigma2*2))
 
  list(ssq = ssq, log.post = log.post)
}

sigma2.update <- function(yobs, xobs, theta, sigma2)
{
  ssq <- model(yobs, xobs, theta, sigma2)$ssq
  nd <- length(yobs)
  sigma2  <- 1 / rgamma(1, shape=(0.001 + nd / 2), scale = 1/(0.001 + ssq / 2))
}

update.theta.Metropolis <- function(yobs, xobs, theta, sigma2, log.post.old, var.jump.matrix)
{
    # -------------------------------------------------------------------------    
    # Generate candidate random point by a mvnormal jumping distribution
    # -------------------------------------------------------------------------

    theta.star <- mvrnorm(1, theta, var.jump.matrix); #  rmvnorm
    log.post.new <- model(yobs, xobs, theta.star, sigma2)$log.post

    r.candid.prob  <- exp(log.post.new - log.post.old);
    p.jump <- 0.0
    if (r.candid.prob > runif(1))
    {
        theta  <-  theta.star
        log.post.old  <-  log.post.new
        p.jump <- min(r.candid.prob,1)
    }
    list(theta=theta, p.jump=p.jump, log.post.old=log.post.old)
}


Metropolis.Algorithm <- function(yobs, xobs, theta, sigma2, var.jump.matrix, tune.const, NoIter=10000)
{
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 # Implementing a hybrid Gibbs-Metropolis sampler
 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 
 np <- length(theta)

 var.jump.matrix <- (tune.const / sqrt(np))^2 * var.jump.matrix

 log.post.old <- model(yobs, xobs, theta, sigma2)$log.post
 
 #************************
 # * Start the MC chain"  *
 # ************************
 res <- matrix(NA, NoIter, np+1, byrow=TRUE)
 colnames(res) <- c(names(theta),"sigma2")

 p.jump <- numeric(NoIter)
 for (i in 1:NoIter)
 {
    sigma2 <- sigma2.update(yobs, xobs, theta, sigma2)
    result <- update.theta.Metropolis(yobs, xobs, theta, sigma2, log.post.old, var.jump.matrix)
    theta <- result$theta
    p.jump[i] <- result$p.jump
    log.post.old <- result$log.post.old
    res[i,] <- c(theta, sigma2)
 }
 list(theta = res, p.jump = p.jump)
}

#Parameter estimates by R's nonlinear least squares function: nls
#      Estimate Std. Error t value Pr(>|t|)    
#Linf 376.98625    4.35601  86.544  < 2e-16 ***
#k      0.16245    0.02076   7.826 4.15e-13 ***
#t0    -1.39175    1.07431  -1.295    0.197    
#---

#Residual standard error: 19.96 on 180 degrees of freedom

#Correlation of Parameter Estimates:
#      Linf     k
#k  -0.8136      
#t0 -0.6769 0.968

var.jump.matrix <-
structure(c(0.000156188435323806, -0.00146906132939133, -0.0112326270158006, 
            -0.00146906132939133, 0.019752582278855, 0.170063983784394,
            -0.0112326270158006,  0.170063983784394, 1.55449294132603),
            .Dim = as.integer(c(3,3)),
            .Dimnames = list(c("Linf", "k", "t0"), c("Linf", "k", "t0")))

# some parameters are transformed to the real line
# here are log(Linf), log(k), t0, sigma^2

theta <- c(5.9,	-1.8,	-1.39)
names(theta) <- c("Linf", "k", "t0")

res <- Metropolis.Algorithm(lvbdata$Lobs, lvbdata$age,
             theta, 403.4, var.jump.matrix, tune.const = 2.4, NoIter = 10000)

# transfer parameters to the original scale
theta <- data.frame(Linf = exp(res$theta[,1]), k =  exp(res$theta[,2]), t0 = res$theta[,3], Sigma2 = res$theta[,4])


mean(res$p.jump)

var.jump.matrix <- var(res$theta)
var.jump.matrix <- var.jump.matrix[1:3,1:3]

# Summaries
mean(theta)
sd(theta)
cor(theta)

# scatter plots
X11()
pairs(theta)

# trace plots
X11()
par(mfrow = c(3,1))
for (j in 1 : 3)
   plot(theta[,j], type ="l", ylab = names(theta)[j])

# autocorrelation
for (j in 1:3)
  print(acf(theta[,j], plot = FALSE))


X11()
par(mfrow = c(2,2))

for (j in 1 : 3)
   acf(theta[,j], plot = TRUE, main = names(theta)[j])



detach(lvbdata)




 
